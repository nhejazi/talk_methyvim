---
title: "The _methyvim_ R Package"
subtitle: "Data-Adaptive Estimation and Inference for Targeted Differential
  Methylation Analysis"
author: "[Nima Hejazi](https://statistics.berkeley.edu/~nhejazi)"
insitution: "University of California, Berkeley"
date: "`r lubridate::now()`"
bibliography: references.bib
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      navigation: {
        scroll: false
      }
---

```{r knitr_setup, echo=FALSE, message=FALSE}
knitr::opts_chunk$set(fig.width = 7, fig.height = 4.5, dpi = 300,
                      fig.cap = "", fig.align = "center")
showtext::showtext_opts(dpi = 300)
```

```{r optoins_setup, include=FALSE}
options(htmltools.dir.version = FALSE)
options(scipen = 999)
```

```{r xaringan-themer, include = FALSE}
library(xaringanthemer)
solarized_light(
  #base_color = "#1c5253",
  header_font_google = google_font("Josefin Sans"),
  text_font_google = google_font("Montserrat", "300", "300i"),
  code_font_google = google_font("Droid Mono")
)
```

```{r refmanager, load_refs, echo=FALSE, cache=FALSE}
library(RefManageR)
BibOptions(check.entries = FALSE,
           bib.style = "authoryear",
           first.inits = TRUE,
           dashed = TRUE,
           max.names = 3,
           cite.style = 'alphabetic',
           style = "markdown",
           hyperlink = FALSE,
           dashed = FALSE)
myBib <- ReadBib("./references.bib", check = FALSE)
```


# Accessing these slides

* Source: https://github.com/nhejazi/talk_methyvim
* Slides: https://bit.ly/bioc_methyvim_2018

???

This slide deck is for a brief (10-15 minute) talk on a recently developed
statistical methodology for using data-adaptive estimates of nonparametric
variable importance measures for differential methylation analysis. This talk
was most recently given at the annual meeting of the [Bioconductor
project](https://bioconductor.org) in Toronto, ON, Canada, in July 2018.

---

# Preview: Summary

* DNA methylation data is _extremely_ high-dimensional -- we can
  collect data on 850K genomic sites with modern arrays!
* Normalization and QC are critical components of properly analyzing
  modern DNA methylation data. There are many choices of technique.
* A relative scarcity of techniques for estimation and inference exists
  -- analyses are often limited to the general linear model.
* Statistical causal inference provides an avenue for answering richer
  scientific questions, especially when combined with modern advances in
  machine learning.

???

We'll go over this summary again at the end of the talk. Hopefully, it will all
make more sense then.

---

# Motivation: Let's meet the data

* Observational study of the impact of disease state on DNA methylation.
* Phenotype-level quantities: $216$ subjects, binary disease status (FASD)
  of each subject, background info on subjects (e.g., sex, age).
* Genomic-level quantities: $\sim 850,000$ CpG sites interrogated using
  the _Infinium MethylationEPIC BeadChip_ by Illumina.
* __Questions__: How do disease status and differential methylation
  relate? Is a coherent biomarker-type signature detectable?

???

* FASD is an abbreviation for Fetal Alcohol Spectrum Disorders.
* We're mostly interested in the interplay between disease and DNA
  methylation.
* In particular, we'd like to construct some kind of importance score for
  CpG sites impacted by the exposure/disease of interest.
* Re: dimensionality, c.f., RNA-seq analyses are $\sim 30,000$ in
  dimension at the gene level.

---

# Data Analysis? Linear Models!

* __First pass:__ For each CpG site $g = 1, \ldots, G$, fit a linear model:
  $$\mathbb{E}[y_g] = X \beta_g$$
* Test the coefficent of interest using a standard t-test:
  $$t_{g} = \frac{\hat{\beta}_{g} - \beta_{g, H_0}}{s_g}$$
* Such models are a matter of convenience: does $\hat{\beta}_{g}$ answer
  our scientific questions? Perhaps not.
* Is consideration being given to whether the data could have been
  generated by a linear model? Perhaps not.

???

* CpG sites are thought to function in networks. Treating them as acting
  independently is _not_ faithful to the underlying biology.
* The linear model is a great starting point for analyses whne the data is
  generated using complex technology -- no need to make the analysis more
  complicated.
* That being said, the data is difficult and expensive to collect, so why
  restrict the scope of the questions we'd like to ask.

---

# Motivation: Science Before Statistics

* __Question:__ What is the effect of disease status on DNA methylation at a
  specific CpG site, controlling for the observed methylation status of the
  neighbors of the given CpG site?
* Treating CpG sites as acting independently is _not_ faithful to the underlying
  biology.
* This means that we should take into account the methylation status of
  neighboring CpG sites when assessing differential methylation at a single
  site.
* This is a coherent scientific question that we can set out to answer
  statistically. It's motivated by the established science and possible to do
  with modern statistical methodology.

---

# Data Analysis Revisited: A Data-Adaptive Approach

* Isolate a subset of CpG sites for which there is cursory evidence of
  differential methylation.
* Assign CpG sites into neighborhoods (e.g., bp distance). If there are
  many neighbors, apply clustering (e.g., _PAM_) to select a subset.
* Estimate _variable importance measure_ (VIM) at each screened CpG
  site, with disease as intervention $A$ and controlling for neighboring CpG
  sites $W$.
* Apply a variant of the Benjamini & Hochberg method for FDR control,
  accounting for initial screening.

???

* Pre-screening is a critical step since we cannot perform computationally
  intensive estimation on all the sites. This is flexible -- just use your
  favorite method (as long as allows a ranking to be made).
* The variable importance step merely comes down to the creation of a
  score. We use TMLE to statistically estimate parameters from causal models.
  The procedure is general enough to accomodate any inference technique.

---

# Pre-Screening: Pick a method, any method

* The estimation procedure is computationally intensive -- apply it only
  to sites that appear promising.
* Consider estimating univariate (linear) regressions of intervention on
  CpG methylation status. Fast, easy.
* Select CpG sites with a marginal p-value below, say, $0.01$. Apply
  data-adaptive procedure to this subset.
* The modeling assumptions do not matter since the we won't be pursuing
  inference under such a model.
* Software implementation is extensible. Users are encouraged to add their
  own. (It's easy!)

???

* We'll be adding to the available routines for pre-screening too! For now, we
  have [`limma`](https://bioconductor.org/packages/limma).
* Several more are on the way.

---

# Too many neighbors? Clustering!

* There are many options: $k$-means, $k$-medoids, etc., as well as many
  algorithmic solutions.
* For convenience, we use __P__artitioning __A__round __M__edoids (PAM), a
  well-established algorithm.
* With limited sample sizes, the number of neighboring sites that may be
  controlled for is limited.
* To faithfully answer the question of interest, choose the neighboring
  sites that are the most representative.
* This is an _optional_ step -- it need only be applied when there
  is a large number of CpG sites in the neighborhood of the target CpG site.
* Nihilistic much? __An Impossiblity Theorem for Clustering__

???

* The number of sites that we can control for is roughly a function of
  sample size. This impacts the definition of the parameter that we estimate,
  and allows enough flexibility to obtain either very local or more regional
  estimates.

---

# Nonparametric Variable Importance

* Let's consider a simple target parameter: the average treatment effect (ATE):
  $$\Psi_g(P_0) = \mathbb{E}_{W,0}[\mathbb{E}_0[Y_g \mid A = 1, W_{-g}] - \mathbb{E}_0[Y_g \mid A = 0, W_{-g}]]$$
* Under certain (untestable) assumptions, interpretable as difference in
  methylation at site $g$ with intervention and, possibly contrary to
  fact, the same under no intervention, controlling for neighboring sites.
* Provides a _nonparametric_ (model-free) measure for those CpG
  sites impacted by a discrete intervention.
* Let the choice of parameter be determined by our scientific question of
  interest.

???

By allowing scientific questions to inform the parameters that we choose
to estimate, we can do a better job of actually answering the questions of
interest to our collaborators. Further, we abandon the need to specify the
functional relationship between our outcome and covariates; moreover, we
can now make use of advances in machine learning.

---

# Targeted Minimum Loss-Based Estimation

* We use _targeted minimum loss-based estimation_ (TMLE), a method
  for inference in semiparametric infinite-dimensional statistical models.
* No need to specify a functional form or assume that we know the true
  data-generating distribution.
* __Asymptotic linearity:__
  $$\Psi_g(P_n^*) - \Psi_g(P_0) = \frac{1}{n} \sum_{i = 1}^{n} IC(O_i) + o_P\left(\frac{1}{\sqrt{n}}\right)$$
* __Limiting distribution:__
  $$\sqrt{n}(\Psi_n - \Psi) \to N(0, Var(D(P_0)))$$
* __Statistical inference:__
  $$\Psi_n \pm z_{\alpha} \cdot \frac{\sigma_n}{\sqrt{n}}$$

???

Under the additional condition that the remainder term $R(\hat{P}^*, P_0)$
decays as $o_P \left( \frac{1}{\sqrt{n}} \right)$, we have that
$\Psi_n - \Psi_0 = (P_n - P_0) \cdot D(P_0) + o_P \left( \frac{1}{\sqrt{n}} \right)$,
which, by a central limit theorem,
establishes a Gaussian limiting distribution for the estimator, with variance
$V(D(P_0))$, the variance of the efficient influence curve (canonical gradient)
when $\Psi$ admits an asymptotically linear representation.

The above implies that $\Psi_n$ is a $\sqrt{n}$-consistent estimator of $\Psi$,
that it is asymptotically normal (as given above), and that it is locally
efficient. This allows us to build Wald-type confidence intervals, where
$\sigma_n^2$ is an estimator of $V(D(P_0))$. The estimator $\sigma_n^2$
may be obtained using the bootstrap or computed directly via
$\sigma_n^2 = \frac{1}{n} \sum_{i = 1}^{n} D^2(\bar{Q}_n^*, g_n)(O_i)$

---

# Corrections for Multiple Testing

* Multiple testing corrections are critical. Without these, we
  systematically obtain misleading results.
* The Benjamini & Hochberg procedure for controlling the False Discovery
  Rate (FDR) is a well-established technique for addressing the multiple
  testing issue.
* We use a modified BH-FDR procedure to account for the pre-screening step
  of the proposed algorithm.
* This modified BH-FDR procedure for multi-stage analyses (FDR-MSA) works
  by adding a p-value of $1.0$ for each site that did not pass pre-screening
  then performs BH-FDR as normal.

???

* Note that $\text{FDR} = \mathbb{E}\left[\frac{V}{R}\right] = \mathbb{E}\left[\frac{V}{R} \mid R > 0 \right] P(R > 0)$.
* BH-FDR procedure: Find $\hat{k} = max\{k: p_{(k)} \leq \frac{k}{M} \cdot \alpha\}$
* FDR-MSA will only incur a loss of power if the initial screening
  excludes variables that would have been rejected by the BH procedure when
  applied to the subset on which estimation was performed.
* BH-FDR control is a rank-based procedure, so we must assume that the
  pre-screening does not disrupt the ranking with respect to the estimation
  subset, which is provably true for screening procedures of a given type.
* MSA controls type I error with any procedure that is a function of only
  the type I error itself --- e.g., FWER. This does not hold for the FDR in
  complete generality.

---

# R/`methyvim`

![](./figs/methyvim_bioc.png)

* Stable release: https://bioconductor.org/packages/methyvim
* Development version: https://github.com/nhejazi/methyvim
* Documentation: https://code.nimahejazi.org/methyvim

???

* Contribute on GitHub: https://github.com/nhejazi/methyvim
* Reach out to us with questions and feature requests.

---

# R/`methyvim`

* Variable importance for discrete interventions.
* Future releases will support continuous interventions.
* Take it for a test drive!

---

# Review: Summary (revisited)

* DNA methylation data is _extremely_ high-dimensional -- we can
  collect data on 850K genomic sites with modern arrays!
* Normalization and QC are critical components of properly analyzing
  modern DNA methylation data. There are many choices of technique.
* A relative scarcity of techniques for estimation and inference exists
  -- analyses are often limited to the general linear model.
* Statistical causal inference provides an avenue for answering richer
  scientific questions, especially when combined with modern advances in
  machine learning.

???

It's always good to include a summary.

---

# Acknowledgements

<u>Science doesn't happen alone</u>:

* University of California, Berkeley:
  * Advisors: Mark J. van der Laan and Alan E. Hubbard
  * Colleagues: Rachael V. Phillips
  * Collaborations: Labs of Martyn Smith and Nina Holland

* Funding source:
  * National Library of Medicine (of NIH): T32-LM012417-02
  * ...
  * ...

---

# References

```{r, 'refs', results='asis', echo=FALSE}
PrintBibliography(myBib, start = 1, end = 10)
```

---

# Review: Summary (revisited...again)

* DNA methylation data is _extremely_ high-dimensional -- we can
  collect data on 850K genomic sites with modern arrays!
* Normalization and QC are critical components of properly analyzing
  modern DNA methylation data. There are many choices of technique.
* A relative scarcity of techniques for estimation and inference exists
  -- analyses are often limited to the general linear model.
* Statistical causal inference provides an avenue for answering richer
  scientific questions, especially when combined with modern advances in
  machine learning.

???

Last time, I promise...

